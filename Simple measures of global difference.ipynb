{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple measures of global difference\n",
    "\n",
    "In general, the simplest statistics are founded on taking a predicted value away from a real value to generate a \"difference\" value, in the case of modelling also known as the \"error\".\n",
    "\n",
    "## Totals\n",
    "\n",
    "Algorithm:\n",
    "Add up the number of events for prediction and reality.\n",
    "Remove one from the other to give a total simple difference, or error.\n",
    "\n",
    "Used by: No one I know?\n",
    "\n",
    "Issues: a very very blunt tool.\n",
    "\n",
    "\n",
    "## Difference statistics\n",
    "\n",
    "With point data, one wants, generally, to convert the points into a value (for example, number of points in an area). This allows two maps of points to be compared (though there are more complicated point-to-point comparison stats).\n",
    "\n",
    "\n",
    "## Kernel density map difference\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "Smooth point events to a raster grid for prediction and reality.\n",
    "Take one map from the other to give a raster map of differences.\n",
    "\n",
    "Used by: ?\n",
    "\n",
    "Advantages: Shows negative and positive errors well.\n",
    "\n",
    "Issues: Algorithm for smoothing. Differences may be positive and negative, making magnitude of overall error hard to see.  \n",
    "Being presented with a set of absolute differences for specific magnitudes of data can make inter-model comparision, especially for different areas with different data magnitudes difficult.\n",
    "\n",
    "\n",
    "## Aggregated areas/zones map difference\n",
    "\n",
    "Algorithm:\n",
    "Using some boundary, aggregate point events to areas for prediction and reality.\n",
    "Take one map from the other to give a zone map of differences.\n",
    "\n",
    "Used by: ?\n",
    "\n",
    "Advantages: shows negative and positive errors well.\n",
    "\n",
    "Issues: Differences may be positive and negative, making magnitude of overall error hard to see.   \n",
    "Being presented with a set of absolute differences for specific magnitudes of data can make inter-model comparision, especially for different areas with different data magnitudes difficult.  \n",
    "Boundaries used can have a significant impact on statistics generated. Google \"Modifiable Areal Unit Problem\". Choice of scale difficult; often driven by policy or desire to match some other government boundaries. Hard to disaggregate results. The correct methodology is to be transparent about biases caused by zoning: see the work by Stan Openshaw on Automatic Zoning, which created zoning to specifically bias statistics (e.g. equal population in each area):\n",
    "https://www.geodata.soton.ac.uk/software/AZTool/  \n",
    "David Martin. [Developing the Automated Zoning Procedure to Reconcile Incompatible Zoning Systems](https://pdfs.semanticscholar.org/b8bb/71655ae188cc00e766c10e4d4af3d828d3bd.pdf).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Disaggregated areas/zones map difference\n",
    "\n",
    "Disaggregating zonal data to points is tricky, but can be done if, for example, smaller scale predictions are needed than validation datasets allow, however it is almost always better to aggregated up the point data.\n",
    "\n",
    "Method 1) Pycnopyllatic reallocation (after Waldo Tobler): take zone data and redistribute it as points, the location of which are weighted by some other dataset, e.g. population distribution. Easiest form is to use the underlying data as a probability surface and try randomly throwing in points, only sticking them successfully when a random number is lower than the probability.\n",
    "\n",
    "Method 2) Microsimulation: starting with the data, allocated it as points such that the location is constrained by other data, either buildings or other demographic data in aggregate statistical form. See:   \n",
    "http://www.geog.leeds.ac.uk/courses/other/crime/microsimulation/practical1.html  \n",
    "https://github.com/MassAtLeeds/FMF/  \n",
    "\n",
    "Once disaggregated, data can be reaggregated to other boundaries or compared with other point data.\n",
    "\n",
    "\n",
    "\n",
    "## Simple global summary statistics derived from difference\n",
    "\n",
    "There are a variety of statistics derived with little additional work from the difference calculations derived by taking the prediction from real data, or vice versa. \n",
    "\n",
    "To calculate the total error one can add up all the differences across a map. However, negatives and positives cancel out, which usually gives an over-optimistic error (though this may be appropriate under some circumstances). Distribution of the differences can be reported.\n",
    "\n",
    "To remove negatives and give an absolute magnitude of error, individual differences can be squared. Sum these to give the Total Squared Error (TSE). Root before summing to give the Total Absolute Error.\n",
    "\n",
    "Divide the TSE by the number of event points to give the Mean Squared Error (MSE).\n",
    "\n",
    "Root the MSE to give the Root Mean Square Error (RMSE). This is the root of the variance, making it the Standard Deviation of the errors.\n",
    "\n",
    "Divide the TAE by the number of event points to give Mean Absolute Error (MAE). \n",
    "\n",
    "Take each difference from the MAE, square to make positive, and sum to give the Variance of the errors. In the case where the data is a sample, rather than the population, divide the TAE by the number of event points − 1 to give an unbiased estimate of the error population variance. Note that this would generally not be appropriate as for errors the Standard Deviation is usually taken as being of the errors, not the variance of the errors from the MAE. Rooting the variance from the MAE gives the population Standard Deviation of the errors -- again, see RMSE from above for a more usual error SD. Using the sample size n, rather than n-1 to divide the TAE gives the sample standard deviation of the errors.  \n",
    "\n",
    "Returning to the RMSE. This is still affected by the magnitude of the predicted and real figures. Therefore, a more usual thing is to divide the RMSE by the range of the data to give the Normalised Root Mean Square Error. This allows model-to-model comparison. However, exactly what consitutes the range is undefined: it could be the range of the prediction; real data; combined data; or data from some larger system. \n",
    "\n",
    "\n",
    "\n",
    "## Differences in flow matrices\n",
    "\n",
    "The same kinds of statistics can be generated from flow matrices, where zones are listed in a table vertically and horizontally, and the cells are filled with the flows from one to the other (either mirroring along the diagonal, if there is no direction recorded, or taking vertical (or horizontal) as the origin zone and horizontal (or vertical) as the destination zone. Predicted flow matrices can be compared with real flow matrices. \n",
    "\n",
    "Flow matrices are often sparse (have many zero value cells), which causes issues with the usefulness of some statistics. A detailed examination of this issue, and a discussion of the best global summary statistics to use can be found in:\n",
    "Knudsen D.C. and Fotheringham A.S. (1986) [Matrix Comparison, Goodness-of-Fit, and Spatial Interaction Modeling](http://irx.sagepub.com/cgi/content/abstract/10/2/127) International Regional Science Review 10: 127–147. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
